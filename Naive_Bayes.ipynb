{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Naive Bayes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes is a classification algorithm that works based on the Bayes theorem. Bayes theorem is used to find the probability of a hypothesis with given evidence.\n",
    "\n",
    "***P(A/B) = P(B/A) * P(A) / P(B)***\n",
    "\n",
    "A is the hypothesis and B is the evidence.\n",
    "\n",
    "P(B|A) is the probability of B given that A is True.\n",
    "\n",
    "P(A) and P(B) is the independent probabilities of A and B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Mutually Exclusive and Exhaustive events***\n",
    "\n",
    "When two events are mutually exclusive, it means they cannot both occur at the same time. But it doesn’t necessarily imply that one of the two events has to happen.\n",
    "\n",
    "When two events are exhaustive, it means that one of them must occur.\n",
    "\n",
    "Think again of a coin toss. The results are mutually exclusive (it will be either heads or tails; it can’t be both on the same flip). And it will be one of the two options — heads and tails are the only possible options (thus they are exhaustive)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding Deck of Cards**\n",
    "\n",
    "A standard deck of playing cards contains 52 cards. Divided equally into two colors \"Red\" and \"Black\". Deck of 52 cards has four suits \"Spades (♠)\", \"Hearts (♥)\", \"Diamonds (♦)\" and \"Clubs (♣)\". Hearts and Diamonds comes in Red color and Spades and Clubs comes in Black color. \n",
    "\n",
    "Each 4 suits contains 13 cards: Ace, 2, 3, 4, 5, 6, 7, 8, 9, 10, Jack, Queen, King."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Probability Concepts***\n",
    "\n",
    "1. Marginal Probability [P(A) or P(B)]: If A is an event, then the marginal probability is the probability of that event occurring, P(A). \n",
    "\n",
    "Example: The probability that a card drawn from a pack is red: \n",
    "P(red) = (13 Hearts)+(13 Diamonds)/Total No of Cards (52) = 26/52 = 0.5\n",
    "\n",
    "2. Joint Probability [P(A∩B) or P(B∩A)]: If A and B are two events then the joint probability of the two events is written as P(A ∩ B).\n",
    "\n",
    "Example: The probability that a card drawn from a pack is red and has the value 4 is P(red and 4): (1 Red Heart)+(1 Red Diamond)/Total Cards (52) = 2/52 = 1/26\n",
    "\n",
    "3. Conditional Probability [P(A/B) or P(B/A)]: If A and B are two events then the conditional probability of A occurring given that B has occurred is written as P(A|B).\n",
    "\n",
    "Example: The probability that a card is a four given that we have drawn a red card is P(4/red) = (1 Red Heart) + (1 Red Daimond)/ (Total Red Cards 26) = 2/26 = 1/13.\n",
    "\n",
    "Linking of all 3 Probabilities with a general multiplication rule as:\n",
    "**P(A ∩ B) = P(A|B) ✕ P(B)**\n",
    "\n",
    "Example: Let A be the event that the card is a 4 and B is the event that the card is red. So \n",
    "\n",
    "P(A) = 4/52 = 1/13\n",
    "P(B) = 26/52 = 1/2\n",
    "\n",
    "The probability that a card is a 4 given that we have drawn a red card is P(4/red) = P(A/B) = 2/26 = 1/13\n",
    "\n",
    "So if we substitute in this formula we get P(A ∩ B) = (1/13) * (1/2) = 1/26 which is the same as the card drawn from a pack is red and has the value 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working of Naive Bayes on Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we are building a classifier that says whether a text is about sports or not. Our training data has 5 sentences as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A great game</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The election was over</td>\n",
       "      <td>Not Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Very clean match</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A clean but forgettable game</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It was a close election</td>\n",
       "      <td>Not Sports</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Text         Tag\n",
       "0                  A great game      Sports\n",
       "1         The election was over  Not Sports\n",
       "2              Very clean match      Sports\n",
       "3  A clean but forgettable game      Sports\n",
       "4       It was a close election  Not Sports"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.DataFrame([])\n",
    "data['Text'] = [\"A great game\", \"The election was over\", \"Very clean match\", \"A clean but forgettable game\", \"It was a close election\"]\n",
    "data['Tag'] = ['Sports', 'Not Sports', 'Sports', 'Sports', 'Not Sports']\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to identify the tag for the given **test_sentence = 'A very close game'**. Since Naive Bayes is a probabilistic classifier, we have to calculate the probability that the sentence \"A very close game\" is Sports and the probability that it’s Not Sports. Then, we take the largest probability as our end result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets transform our requirement as mathematically as:\n",
    "\n",
    "P(Sports/'A very close game') and P(Not Sports/'A very close game')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets generate the features for the given text data by using **Word Frequencies**. That is, we ignore word order and sentence construction, treating every document as a set of the words it contains. Our features will be the counts of each of these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of words in Sports:  8\n",
      "great          1.0\n",
      "game           2.0\n",
      "A              2.0\n",
      "clean          2.0\n",
      "Very           1.0\n",
      "match          1.0\n",
      "but            1.0\n",
      "forgettable    1.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "### Sports\n",
    "sports_vocab = data[data['Tag'] == 'Sports']['Text'].apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0)\n",
    "print('Total no of words in Sports: ', len(sports_vocab))\n",
    "print(sports_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of words in Not Sports:  7\n",
      "The         1.0\n",
      "over        1.0\n",
      "was         2.0\n",
      "election    2.0\n",
      "a           1.0\n",
      "It          1.0\n",
      "close       1.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "### Not Sports\n",
    "not_sports_vocab = data[data['Tag'] == 'Not Sports']['Text'].apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0)\n",
    "print('Total no of words in Not Sports: ', len(not_sports_vocab))\n",
    "print(not_sports_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of words:  15\n",
      "great          1.0\n",
      "game           2.0\n",
      "A              2.0\n",
      "The            1.0\n",
      "over           1.0\n",
      "was            2.0\n",
      "election       2.0\n",
      "clean          2.0\n",
      "Very           1.0\n",
      "match          1.0\n",
      "but            1.0\n",
      "forgettable    1.0\n",
      "a              1.0\n",
      "It             1.0\n",
      "close          1.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "### Not Sports\n",
    "combined_vocab = data['Text'].apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0)\n",
    "print('Total no of words: ', len(combined_vocab))\n",
    "print(combined_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Apply Bayes Therom\n",
    "\n",
    "P(A/B) = P(B/A) * P(A) / P(B)\n",
    "\n",
    "In our case, we have \n",
    "\n",
    "P(Sports | a very close game) = P(a very close game | Sports) * P(Sports) / P(a very close game) \n",
    "\n",
    "and\n",
    "\n",
    "P(Not Sports | a very close game) = P(a very close game | Not Sports) * P(Not Sports) / P(a very close game)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From P(a very close game) we have to calculate the count of occurances of the given sentence across the rows and then divide it by total no of rows will gives the result.\n",
    "\n",
    "There's a problem though: “A very close game” doesn’t appear in our training data, so this probability is zero. Unless every sentence that we want to classify appears in our training data, the model won’t be very useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here comes the ***Naive*** part of the algorithm by assuming every word in the given sentences as independant of other ones which can be written as \n",
    "\n",
    "P(a very close game) = P(a) * P(very) * P(close) * P(game)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we appy this step to the above we will get:\n",
    "    \n",
    "    P(a very close game | Sports) = P(a/Sports) * P(very/Sports) * P(close/Sports) * P(game/Sports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(Sports) = 3/5\n",
    "P(Not Sports) = 2/5\n",
    "\n",
    "P(a/Sports) = 2/8 \n",
    "P(very/Sports) = 1/8\n",
    "P(close/Sports) = 0/8 = 0\n",
    "P(game/Sports) = 2/8\n",
    "\n",
    "So P(a very close game | Sports) = 2/8 * 1/8 * 0 * 2/8 = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Doing things this way simply doesn't give us any information at all, so we have to find a way around."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To handle scenarios like this we have to use **Laplace smoothing** technique. we add 1 to every count so it’s never zero. To balance this, we add the number of possible words to the divisor, so the division will never be greater than 1. In our case it will be (8+7) = 15 and it can be applied as below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(game/Sports) = (2+1)/(8+15) = 3/23\n",
    "P(close/Sports) = (0+1)/(8+15) = 1/23\n",
    "P(very/Sports) = (1+1)/(8+15) = 2/23\n",
    "P(a/Sports) = (0+1)/(8+15) = 3/23\n",
    "\n",
    "P(a very close game | Sports) = 3/23 * 1/23 * 2/23 * 1/23 = 6/279841 = **0.000021440**\n",
    "\n",
    "P(game/Not Sports) = (0+1)/(7+15) = 1/22\n",
    "P(close/Not Sports) = (1+1)/(7+15) = 2/22\n",
    "P(very/Not Sports) = (0+1)/(7+15) = 1/22\n",
    "P(a/Not Sports) = (1+1)/(7+15) = 2/22\n",
    "\n",
    "P(a very close game | Not Sports) = 4/234256 = **0.00001707**\n",
    "\n",
    "**0.000021440** > **0.00001707**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So from this we can say that our classifier concludes this as Sports Tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many things that can be done to improve this basic model. These techniques allow Naive Bayes to perform at the same level as more advanced methods. Some of these techniques are:\n",
    "\n",
    "1. Removing stopwords.\n",
    "2. Lemmatizing words. \n",
    "3. Using n-grams.\n",
    "4. Using TF-IDF."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
